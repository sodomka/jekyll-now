---
layout: post
title: Tentative EC 2016 Game Plan
author: "Eric Sodomka"
date: "May 5, 2016"
output: 
  html_document
---

(NOTE: If you have stumbled upon this post, you're reading a rough draft that I'm playing around with for setting up this blog. Everything here is subject to change. Continue reading at your own peril.)

My goal is to read/summarize every EC paper by the time of the conference.


```{r echo = FALSE}
ec_start_date <- as.Date('2016-07-24')
acceptance_date <- as.Date('2016-05-10')
camera_ready_date <- as.Date('2016-05-30')
start_date <- as.Date('2016-05-30')
# EC2015 acceptances: 72 (http://dl.acm.org/citation.cfm?id=2764468)
num_papers <- 72 

# assuming I start reviewing from acceptance date, and end with some buffer
# before the EC start date.
days_buffer <- 7
days_available <- as.numeric(ec_start_date - start_date) - days_buffer
papers_per_day <- num_papers / days_available
```

* EC 2016 start date: `r ec_start_date`.
* Accepted papers announcement date: `r acceptance_date`. 
* Camera-ready due date: `r camera_ready_date`.
* Day I'll start reading: `r start_date`.
* I'll add in a `r days_buffer`-day buffer for delays or handling other urgent, unrelated things that might come up.
* Number of accepted papers: `r num_papers`.

In total, I'll have about **`r days_available` days** for reading/summarizing these papers. This means I'll have to average about **`r round(papers_per_day, 2)` papers per day** to get through them all.

Seeing the number, this actually seems pretty daunting, given I also have a full-time job. For each paper, I should make a quick list of what I want:

* What they did (something slightly more detailed than the abstract, but in my own words and in relation to other work I'm familiar with).
* The main contribution of this work.
* Any take-aways to investigate further, that might be applicable (e.g., proof techniques).
* Some questions for the authors. (Perhaps basic clarifying questions, if I'm unfamiliar with the area.)
* A list of papers to look at for background reading.

## An iterative approach

```{r}
minutes_per_shallow_dive <- 30
minutes_per_medium_dive <- 120
minutes_per_deep_dive <- 4 * 60
```

I'll do each of these steps for each paper:

1. Read title and list of authors. Just to get a big-picture view of the publication areas.
1. Shallow dive. Read the abstract. Look at all figures / theorems. Try to identify the main result. Read intro/conclusion.
1. Medium dive. Skim the entire paper, skipping technical details. Make note of what I skipped that might be worth reading later.
1. Deep dive. Read end-to-end, including technical details except for proofs.
1. Deepest dive. Dig into proofs. Write some R code that captures some main point of the paper. See if I can reproduce in some simple version.


## Some TODOs

* Setup comments section so people can add their own insights / correct me where I'm wrong.
* Make sure I can write LaTeX in posts
* Make sure I can add images to posts
* Try a practice version with a few papers from EC 2015. (Both for figuring out how much time things take, as well as working out any kinks in the publishing process.)
* Figure out some ordering for reading papers that makes sense (by topic?)

